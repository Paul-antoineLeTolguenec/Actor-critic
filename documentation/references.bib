 @InProceedings{pmlr-v48-mniha16, title = {Asynchronous Methods for Deep Reinforcement Learning}, 
 author = {Volodymyr Mnih and Adria Puigdomenech Badia and Mehdi Mirza and Alex Graves and Timothy Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu}, 
 booktitle = {Proceedings of The 33rd International Conference on Machine Learning}, pages = {1928--1937}, year = {2016}, editor = {Maria Florina Balcan and Kilian Q. Weinberger}, volume = {48}, series = {Proceedings of Machine Learning Research}, address = {New York, New York, USA}, month = {20--22 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v48/mniha16.pdf}, url = {http://proceedings.mlr.press/v48/mniha16.html}, abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.} } 

@misc{avoidance,
      title={Autonomous Obstacle Avoidance by Learning Policies for Reference Modification}, 
      author={Benjamin Evans and Hendrik W. Jordaan and Herman A. Engelbrecht},
      year={2021},
      eprint={2102.11042},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}
 @article{DBLP,
  author    = {Guillaume Matheron and
               Nicolas Perrin and
               Olivier Sigaud},
  title     = {The problem with {DDPG:} understanding failures in deterministic environments
               with sparse rewards},
  journal   = {CoRR},
  volume    = {abs/1911.11679},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.11679},
  archivePrefix = {arXiv},
  eprint    = {1911.11679},
  timestamp = {Tue, 03 Dec 2019 20:41:07 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-11679.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{ANN,
  title={Artificial Neural Network},
  author={Wang SC.},
  url={https://link.springer.com/chapter/10.1007/978-1-4615-0377-4_5},
  year={2003},
  publisher={The Springer International Series in Engineering and Computer Science,}
}

@article{LSTM,
  author    = {Ralf C. Staudemeyer and
               Eric Rothstein Morris},
  title     = {Understanding {LSTM} - a tutorial into Long Short-Term Memory Recurrent
               Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1909.09586},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.09586},
  archivePrefix = {arXiv},
  eprint    = {1909.09586},
  timestamp = {Tue, 24 Sep 2019 11:33:51 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-09586.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{CONT,

  author={H. {van Hasselt} and M. A. {Wiering}},

  booktitle={2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning}, 

  title={Reinforcement Learning in Continuous Action Spaces}, 

  year={2007},

  volume={},

  number={},

  pages={272-279},

  doi={10.1109/ADPRL.2007.368199}}

@article{Actor,
title = {Natural actor–critic algorithms},
journal = {Automatica},
volume = {45},
number = {11},
pages = {2471-2482},
year = {2009},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2009.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0005109809003549},
author = {Shalabh Bhatnagar and Richard S. Sutton and Mohammad Ghavamzadeh and Mark Lee},
keywords = {Actor–critic reinforcement learning algorithms, Policy-gradient methods, Approximate dynamic programming, Function approximation, Two-timescale stochastic approximation, Temporal difference learning, Natural gradient},
abstract = {We present four new reinforcement learning algorithms based on actor–critic, natural-gradient and function-approximation ideas, and we provide their convergence proofs. Actor–critic reinforcement learning methods are online approximations to policy iteration in which the value-function parameters are estimated using temporal difference learning and the policy parameters are updated by stochastic gradient descent. Methods based on policy gradients in this way are of special interest because of their compatibility with function-approximation methods, which are needed to handle large or infinite state spaces. The use of temporal difference learning in this way is of special interest because in many applications it dramatically reduces the variance of the gradient estimates. The use of the natural gradient is of interest because it can produce better conditioned parameterizations and has been shown to further reduce variance in some cases. Our results extend prior two-timescale convergence results for actor–critic methods by Konda and Tsitsiklis by using temporal difference learning in the actor and by incorporating natural gradients. Our results extend prior empirical studies of natural actor–critic methods by Peters, Vijayakumar and Schaal by providing the first convergence proofs and the first fully incremental algorithms.}
}

@misc{A3C,
      title={Asynchronous Methods for Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
      year={2016},
      eprint={1602.01783},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{vanishing,
  author    = {Yuhuang Hu and
               Adrian E. G. Huber and
               Jithendar Anumula and
               Shih{-}Chii Liu},
  title     = {Overcoming the vanishing gradient problem in plain recurrent networks},
  journal   = {CoRR},
  volume    = {abs/1801.06105},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.06105},
  archivePrefix = {arXiv},
  eprint    = {1801.06105},
  timestamp = {Tue, 18 Sep 2018 12:35:49 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1801-06105.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DQN,
  author    = {Zhuoran Yang and
               Yuchen Xie and
               Zhaoran Wang},
  title     = {A Theoretical Analysis of Deep Q-Learning},
  journal   = {CoRR},
  volume    = {abs/1901.00137},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.00137},
  archivePrefix = {arXiv},
  eprint    = {1901.00137},
  timestamp = {Thu, 31 Jan 2019 13:52:49 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-00137.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
